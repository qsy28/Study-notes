{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据的特征抽取\n",
    "\n",
    "    - 将文本、非连续型等数据转化为数值\n",
    "    - 使用包：`sklearn.feature_extraction`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 字典数据特征抽取 `sklearn.feature_extraction.DictVectorizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`DictVectorizer(sparse=True,…)`** #sparse为一种数据类型\n",
    "\n",
    "\\----------------------------\n",
    "\n",
    "DictVectorizer.fit_transform(X) \n",
    "\n",
    "X:字典或者包含字典的迭代器\n",
    "\n",
    "返回值：返回sparse矩阵\n",
    "\n",
    "\\----------------------------\n",
    "\n",
    "DictVectorizer.inverse_transform(X)\n",
    "\n",
    "X:array数组或者sparse矩阵\n",
    "\n",
    "返回值:转换之前数据格式\n",
    "\n",
    "\\----------------------------\n",
    "\n",
    "DictVectorizer.get_feature_names()\n",
    "\n",
    "返回类别名称\n",
    "\n",
    "\\----------------------------\n",
    "\n",
    "DictVectorizer.transform(X)\n",
    "\n",
    "按照原先的标准转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1.0\n",
      "  (0, 3)\t100.0\n",
      "  (1, 0)\t1.0\n",
      "  (1, 3)\t60.0\n",
      "  (2, 2)\t1.0\n",
      "  (2, 3)\t30.0\n"
     ]
    }
   ],
   "source": [
    "dic=[{'city': '北京','temperature':100},{'city': '上海','temperature':60},{'city': '深圳','temperature':30}]\n",
    "\n",
    "\n",
    "# 实例化一个DictVectorizer\n",
    "dv = DictVectorizer()\n",
    "sp = dv.fit_transform(dic)\n",
    "print(sp) #sparse的数据格式： (x,y)表示位置，位置上为0的不显示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   1.   0. 100.]\n",
      " [  1.   0.   0.  60.]\n",
      " [  0.   0.   1.  30.]]\n"
     ]
    }
   ],
   "source": [
    "dv1 = DictVectorizer(sparse=False)\n",
    "data = dv1.fit_transform(dic)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['city=上海', 'city=北京', 'city=深圳', 'temperature']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 理解data该二位数组的含义\n",
    "dv1.get_feature_names() #四个特征值分别对应data数组的列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 将city的属性值转化为[0,1,0]这样的格式被称为：one-hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'city=北京': 1.0, 'temperature': 100.0},\n",
       " {'city=上海': 1.0, 'temperature': 60.0},\n",
       " {'city=深圳': 1.0, 'temperature': 30.0}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv1.inverse_transform(data) #将转化成的二位数组重新变为字典格式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本特征抽取 \n",
    "### `sklearn.feature_extraction.text.CountVectorizer`  and `sklearn.feature_extraction.text.TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**count:**\n",
    "\n",
    "    1. 统计所有文档中出现的所有词语（单个英文单词、汉字不统计）\n",
    "    2. 计算各文档中各词出现的频次\n",
    "\n",
    "**ti-idf：**\n",
    "    \n",
    "    1. 统计所有文档中出现的所有词语（单个英文单词、汉字不统计）\n",
    "    2. 计算各文档中各词出现的频次（tf）\n",
    "    3. 计算逆文档频率（idf）\n",
    "    4. 计算tf-idf值：tf*idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$idf = \\log(\\frac{总文档数量}{该词出现的文档数量})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。\n",
    "TF-IDF作用：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CountVectorizer(max_df=1.0,min_df=1,…)**\n",
    "\n",
    "返回词频矩阵\n",
    "\n",
    "\\---------------------\n",
    "\n",
    "CountVectorizer.fit_transform(X,y)  \n",
    "\n",
    "X:文本或者包含文本字符串的可迭代对象\n",
    "\n",
    "返回值：返回sparse矩阵\n",
    "\n",
    "\\---------------------\n",
    "\n",
    "CountVectorizer.inverse_transform(X)\n",
    "\n",
    "X:array数组或者sparse矩阵\n",
    "\n",
    "返回值:转换之前数据格式\n",
    "\n",
    "\\---------------------\n",
    "\n",
    "CountVectorizer.get_feature_names()\n",
    "\n",
    "返回值:单词列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 1)\t2\n",
      "  (0, 6)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 5)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "etext = [\"life is is short,i like python\",\"life is too long,i dislike python\"]\n",
    "# 实例化一个countvectorizer\n",
    "cv = CountVectorizer() #没有sparse参数，返回值只有sparse\n",
    "data1 = cv.fit_transform(etext) #函数通过空格分隔单词，因此中文必须先进行分词\n",
    "print(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2 1 1 0 1 1 0]\n",
      " [1 1 1 0 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# 将sparse转成ndarray\n",
    "print(data1.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dislike', 'is', 'life', 'like', 'long', 'python', 'short', 'too']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 打印出特征\n",
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TfidfVectorizer(stop_words=None,…)**\n",
    "\n",
    "返回词的权重矩阵\n",
    "\n",
    "\\---------------------\n",
    "\n",
    "TfidfVectorizer.fit_transform(X,y) \n",
    "\n",
    "X:文本或者包含文本字符串的可迭代对象\n",
    "\n",
    "返回值：返回sparse矩阵\n",
    "\n",
    "\\---------------------\n",
    "\n",
    "TfidfVectorizer.inverse_transform(X)\n",
    "\n",
    "X:array数组或者sparse矩阵\n",
    "\n",
    "返回值:转换之前数据格式\n",
    "\n",
    "\\---------------------\n",
    "\n",
    "TfidfVectorizer.get_feature_names()\n",
    "\n",
    "返回值:单词列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理中文\n",
    "\n",
    "text1 = '今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。'\n",
    "text2 = '我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。'\n",
    "text3 = '如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "con1 = jieba.cut(text1)\n",
    "con2 = jieba.cut(text2)\n",
    "con3 = jieba.cut(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。\n"
     ]
    }
   ],
   "source": [
    "cut1 = ' '.join(con1) #用空格联结列表里的元素，变成字符串\n",
    "print(cut1)\n",
    "cut2 = ' '.join(con2)\n",
    "cut3 = ' '.join(con3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b1914f5dc09e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 实例化一个tfidfvectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdata2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcut1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcut2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcut3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#data2为sparse数据格式\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "# 实例化一个tfidfvectorizer\n",
    "tv = TfidfVectorizer()\n",
    "data2 = tv.fit_transform([cut1,cut2,cut3]) #data2为sparse数据格式\n",
    "print(data2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['一种', '不会', '不要', '之前', '了解', '事物', '今天', '光是在', '几百万年', '发出', '取决于', '只用', '后天', '含义', '大部分', '如何', '如果', '宇宙', '我们', '所以', '放弃', '方式', '明天', '星系', '晚上', '某样', '残酷', '每个', '看到', '真正', '秘密', '绝对', '美好', '联系', '过去', '这样']\n"
     ]
    }
   ],
   "source": [
    "print(tv.get_feature_names())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据的特征预处理 `sklearn.preprocessing`\n",
    "\n",
    "- 通过特定的统计方法（数学方法）将数据转换成算法要求的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 归一化"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**定义：**\n",
    "\n",
    "$X' = \\frac{x - min}{max - min}$\n",
    "\n",
    "$X'' = X' * (max_{new} - min_{new}) + min_{new}$\n",
    "\n",
    "$max_{new}，min_{new}分别为指定区间值默认min_{new}为1，min_{new}为0。$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**缺点：容易受异常点影响。**\n",
    "\n",
    "最大值与最小值非常容易受异常点影响，所以这种方法鲁棒性较差，只适合传统精确小数据场景。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**API:`sklearn.preprocessing.MinMaxScaler`**\n",
    "\n",
    "MinMaxScalar(feature_range=(0,1)…)\n",
    "\n",
    "每个特征缩放到给定范围(默认[0,1])\n",
    "\n",
    "\\--------------------------\n",
    "\n",
    "MinMaxScalar.fit_transform(X)   \n",
    "\n",
    "X:numpy array格式的数据\n",
    "\n",
    "返回值：转换后的形状相同的array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 1.        , 1.        , 0.83333333],\n",
       "       [0.5       , 0.5       , 0.6       , 1.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#实例化\n",
    "mm = MinMaxScaler()\n",
    "data = mm.fit_transform([[90,2,10,40],[60,4,15,45],[75,3,13,46]])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 标准化"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**定义：**\n",
    "\n",
    "$X'=\\frac{x-\\mu}{\\delta}$\n",
    "\n",
    "通过对原始数据进行变换把数据变换到均值为0,方差为1范围内!\n",
    "\n",
    "对异常值的敏感度低于归一化"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**API:`sklearn.proprecessing.StandardScaler`**\n",
    "\n",
    "StandardScaler(…)\n",
    "\n",
    "处理之后每列来说所有数据都聚集在均值0附近方差为1\n",
    "\n",
    "\\----------------------------------\n",
    "\n",
    "StandardScaler.fit_transform(X,y)    \n",
    "\n",
    "X:numpy array格式的数据\n",
    "\n",
    "返回值：转换后的形状相同的array\n",
    "\n",
    "\\----------------------------------\n",
    "\n",
    "StandardScaler.mean_\n",
    "\n",
    "原始数据中每列特征的平均值\n",
    "\n",
    "\\----------------------------------\n",
    "\n",
    "StandardScaler.std_\n",
    "\n",
    "原始数据每列特征的方差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.06904497, -1.35873244,  0.98058068],\n",
       "       [-0.26726124,  0.33968311,  0.39223227],\n",
       "       [ 1.33630621,  1.01904933, -1.37281295]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#实例化\n",
    "ss = StandardScaler()\n",
    "\n",
    "data = ss.fit_transform([[ 1., -1., 3.],[ 2., 4., 2.],[ 4., 6., -1.]])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 缺失值处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 删除\n",
    "\n",
    "- 插补`sklearn.propercessing.Imputer`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Imputer(missing_values='NaN',strategy='mean',axis=0)`\n",
    "\n",
    "完成缺失值插补,axis默认为0，表示按列插补\n",
    "\n",
    "\\------------------------------------\n",
    "\n",
    "Imputer.fit_transform(X,y) \n",
    "\n",
    "X:numpy array格式的数据\n",
    "\n",
    "返回值：转换后的形状相同的array"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***np.nan***\n",
    "\n",
    "numpy的数组中可以使用np.nan/np.NaN来代替缺失值，属于float类型\n",
    "\n",
    "如果是文件中的一些缺失值，可以替换成nan，通过np.array转化成float型的数组即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 分类无序变量：one-hot编码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 数据的降维\n",
    "\n",
    "- 特征选择\n",
    "\n",
    "- 主成分分析PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 特征选择"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**定义：**\n",
    "\n",
    "特征选择就是单纯地从提取到的所有特征中选择部分特征作为训练集特征，特征在选择前和选择后可以改变值、也不改变值，但是选择后的特征维数肯定比选择前小，毕竟我们只选择了其中的一部分特征。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**方法：**\n",
    "\n",
    "1. Filter(过滤式):VarianceThreshold\n",
    "2. Embedded(嵌入式)：正则化、决策树\n",
    "3. Wrapper(包裹式)\n",
    "4. 神经网络"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter:**\n",
    "\n",
    "**`sklearn.feature_selection.VarianceThreshold`**\n",
    "\n",
    "VarianceThreshold(threshold = 0.0)\n",
    "\n",
    "删除所有低方差特征\n",
    "\n",
    "\\--------------------------------\n",
    "\n",
    "Variance.fit_transform(X,y)    \n",
    "\n",
    "X:numpy array格式的数据\n",
    "\n",
    "返回值：训练集差异低于threshold的特征将被删除。默认值是保留所有非零方差特征，即删除所有样本中具有相同值的特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [4],\n",
       "       [1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "#实例化\n",
    "vt = VarianceThreshold(threshold=1.0)\n",
    "\n",
    "data = vt.fit_transform([[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]])\n",
    "\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 降维**`sklearn.decomposition`**\n",
    "\n",
    "- 是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息。\n",
    "- PCA  LDA（线性判别分析）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`sklearn.decomposition.PCA`**\n",
    "\n",
    "PCA(n_components=None) **当 n 为小数，表示需要保留的数据集信息量，为整数时，表示留下的我维度，一般使用小数**\n",
    "\n",
    "将数据分解为较低维数空间\n",
    "\n",
    "\\---------------------------\n",
    "\n",
    "PCA.fit_transform(X) \n",
    "\n",
    "X:numpy array格式的数据\n",
    "\n",
    "返回值：转换后指定维度的array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.13587302e-16,  3.82970843e+00],\n",
       "       [-5.74456265e+00, -1.91485422e+00],\n",
       "       [ 5.74456265e+00, -1.91485422e+00]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.9)\n",
    "\n",
    "data = pca.fit_transform([[2,8,4,5],[6,3,0,8],[5,4,9,1]])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
