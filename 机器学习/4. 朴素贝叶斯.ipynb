{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 概率基础\n",
    "\n",
    "**联合概率：**包含多个条件，且所有条件同时成立的概率，记作：$𝑃(𝐴,𝐵)$\n",
    "\n",
    "**条件概率：**就是事件A在另外一个事件B已经发生条件下的发生概率记作：$𝑃(𝐴|𝐵)$\n",
    "\n",
    "特性：$P(A1,A2|B) = P(A1|B)P(A2|B)$，注意：此条件概率的成立，是由于**A1,A2相互独立**的结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 贝叶斯决策论\n",
    "\n",
    "$\\lambda_{ij}$是将一个真实标记为$c_j$的样本误分类为$c_i$所产生的损失。于是将样本$x$分类为$c_i$所产生的期望损失：\n",
    "\n",
    "$$ R(c_i|x) = \\sum_{j=1}^{N}\\lambda_{ij}P(c_j|x) $$\n",
    "\n",
    "我们的目标是最小化总体风险：\n",
    "\n",
    "$$ R(h) = E_x[R(h(x)|x)] $$\n",
    "\n",
    "显然对于每个样本$x$，若$h$能最小化条件风险$R(h(x)|x)$，则总体风险$R(h)$也就最小。\n",
    "\n",
    "于是有**贝叶斯准则：**<span class=\"girk\"><span class=\"mark\">为最小化总体风险，只需在每个样本上选择了那个能使条件风险$R(c|x)$最小的类别标记。</span></span>\n",
    "\n",
    "$$ h^*(x) = \\underset {c \\epsilon Y}{arg min}R(c|x) $$\n",
    "\n",
    "$h^*$为贝叶斯最优分类器，$R(h^*)$为贝叶斯风险，$1-R(h^*)$反应了分类器所能达到的最好性能，即机器学习所产生的模型精度理论上限。\n",
    "\n",
    "具体来说，若目标是最小化分类错误率，则判误损失$\\lambda_{ij}$可写为：\n",
    "\n",
    "\n",
    "$$\n",
    "\\lambda_{ij}=\n",
    "\\begin{cases}\n",
    "0 & \\text{i = j} \\\\\n",
    "1 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "此时的条件风险 $ R(c|x) = 1 - P(c|x) $\n",
    "\n",
    "于是，最小化分类错误率的贝叶斯最优分类器为：\n",
    "\n",
    "$$ h^*(x) = \\underset {c \\epsilon Y}{argmax} P(c|x) $$\n",
    "\n",
    "对于生成模型，考虑： $ P(c|x) = \\frac {P(x,c)} {P(x)} $\n",
    "\n",
    "根据<span class=\"mark\">**贝叶斯定理**</span>，有：\n",
    "\n",
    "$$ P(c|x) = \\frac {P(c)*P(x|c)} {P(x)} $$\n",
    "\n",
    "于是估计$ P(c|x) $ 问题转化为如何基于训练数据 $D$ 来估计先验 $P(c)$和似然 $P(x|c)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 朴素贝叶斯\n",
    "\n",
    "**朴素贝叶斯分类器采用了<span class=\"mark\">“属性条件独立性假设”</span>：对已知类别，假设所有属性相互独立。**\n",
    "\n",
    "即各特征值独立，$P(x|c) = \\prod_{i=1}^d P(x_i | c)$, $d$为属性数目。\n",
    "\n",
    "于是有朴素贝叶斯分类器表达式：\n",
    "\n",
    "$$ h_{nb}^*(x) = \\underset {c \\epsilon Y}{argmax} P(c) \\prod_{i=1}^d P(x_i | c)$$\n",
    "\n",
    "$$ P(c) = \\frac {|D_c|}{|D|} $$\n",
    "\n",
    "$$ P(x_i | c) = \\frac {|D_{c,x_i}|}{|D_c|} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 拉普拉斯平滑\n",
    "\n",
    "$$ P'(c) = \\frac {|D_c|+\\alpha} {|D|+ \\alpha N} $$\n",
    "\n",
    "$$ P'(x_i | c) = \\frac {|D_{c,x_i}|+\\alpha} {|D_c|+ \\alpha N_i} $$\n",
    "\n",
    "$\\alpha$为拉普拉斯系数，一般为1\n",
    "\n",
    "$N$ 为训练集D中的类别数\n",
    "\n",
    "$N_i$ 为属性值$i$的取值数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 基于朴素贝叶斯的文本分类\n",
    "\n",
    "\n",
    "$$ 𝑃(𝐶|𝑊)=\\frac{𝑃(𝑊|𝐶)𝑃(𝐶)}{𝑃(𝑊)} $$\n",
    "\n",
    "注：$W$为给定文档的特征值(频数统计,预测文档提供)，$c$为文档类别\n",
    "\n",
    "\n",
    "公式可以理解为：\n",
    "\n",
    "$$ 𝑃(𝐶|𝐹_1,𝐹_2,…)=\\frac{𝑃(𝐹_1,𝐹_2,… |𝐶)𝑃(𝐶)}{𝑃(𝐹_1,𝐹_2,…)}$$\n",
    "\n",
    "其中$c$可以是不同类别\n",
    "\n",
    "$𝑃(𝐶)$：每个文档类别的概率(某文档类别词数／总文档词数)\n",
    "\n",
    "$𝑃(𝑊|𝐶)$：给定类别下特征（被预测文档中出现的词）的概率\n",
    "\n",
    "计算方法：\n",
    "\n",
    "$𝑃(𝐹_1|𝐶)=𝑁_𝑖/𝑁$\t（训练文档中去计算）\n",
    "\n",
    "$𝑁_𝑖$为该$𝐹_1$词在$C$类别所有文档中出现的次数\n",
    "\n",
    "$N$为所属类别$C$下的文档所有词出现的次数和"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. API调用\n",
    "\n",
    "#### `sklearn.naive_bayes.MultinomialNB(alpha = 1.0)`\n",
    "\n",
    "alpha：拉普拉斯平滑系数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 优缺点\n",
    "\n",
    "- 优点：\n",
    "\n",
    "    - 朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率。\n",
    "\n",
    "    - 对缺失数据不太敏感，算法也比较简单，常用于文本分类。\n",
    "\n",
    "    - 分类准确度高，速度快\n",
    "\n",
    "- 缺点：\n",
    "\n",
    "    - 需要知道$P(F_1,F_2,…|C)$，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 案例：新闻分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T01:25:50.353047Z",
     "start_time": "2021-04-10T01:25:50.335086Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T01:20:17.780685Z",
     "start_time": "2021-04-10T01:20:17.463815Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  3, 17, ...,  3,  1,  7])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = fetch_20newsgroups(subset='all')\n",
    "news.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T01:20:22.902020Z",
     "start_time": "2021-04-10T01:20:22.884034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _20newsgroups_dataset:\n",
      "\n",
      "The 20 newsgroups text dataset\n",
      "------------------------------\n",
      "\n",
      "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
      "20 topics split in two subsets: one for training (or development)\n",
      "and the other one for testing (or for performance evaluation). The split\n",
      "between the train and test set is based upon a messages posted before\n",
      "and after a specific date.\n",
      "\n",
      "This module contains two loaders. The first one,\n",
      ":func:`sklearn.datasets.fetch_20newsgroups`,\n",
      "returns a list of the raw texts that can be fed to text feature\n",
      "extractors such as :class:`~sklearn.feature_extraction.text.CountVectorizer`\n",
      "with custom parameters so as to extract feature vectors.\n",
      "The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n",
      "returns ready-to-use features, i.e., it is not necessary to use a feature\n",
      "extractor.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    =================   ==========\n",
      "    Classes                     20\n",
      "    Samples total            18846\n",
      "    Dimensionality               1\n",
      "    Features                  text\n",
      "    =================   ==========\n",
      "\n",
      "Usage\n",
      "~~~~~\n",
      "\n",
      "The :func:`sklearn.datasets.fetch_20newsgroups` function is a data\n",
      "fetching / caching functions that downloads the data archive from\n",
      "the original `20 newsgroups website`_, extracts the archive contents\n",
      "in the ``~/scikit_learn_data/20news_home`` folder and calls the\n",
      ":func:`sklearn.datasets.load_files` on either the training or\n",
      "testing set folder, or both of them::\n",
      "\n",
      "  >>> from sklearn.datasets import fetch_20newsgroups\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train')\n",
      "\n",
      "  >>> from pprint import pprint\n",
      "  >>> pprint(list(newsgroups_train.target_names))\n",
      "  ['alt.atheism',\n",
      "   'comp.graphics',\n",
      "   'comp.os.ms-windows.misc',\n",
      "   'comp.sys.ibm.pc.hardware',\n",
      "   'comp.sys.mac.hardware',\n",
      "   'comp.windows.x',\n",
      "   'misc.forsale',\n",
      "   'rec.autos',\n",
      "   'rec.motorcycles',\n",
      "   'rec.sport.baseball',\n",
      "   'rec.sport.hockey',\n",
      "   'sci.crypt',\n",
      "   'sci.electronics',\n",
      "   'sci.med',\n",
      "   'sci.space',\n",
      "   'soc.religion.christian',\n",
      "   'talk.politics.guns',\n",
      "   'talk.politics.mideast',\n",
      "   'talk.politics.misc',\n",
      "   'talk.religion.misc']\n",
      "\n",
      "The real data lies in the ``filenames`` and ``target`` attributes. The target\n",
      "attribute is the integer index of the category::\n",
      "\n",
      "  >>> newsgroups_train.filenames.shape\n",
      "  (11314,)\n",
      "  >>> newsgroups_train.target.shape\n",
      "  (11314,)\n",
      "  >>> newsgroups_train.target[:10]\n",
      "  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\n",
      "\n",
      "It is possible to load only a sub-selection of the categories by passing the\n",
      "list of the categories to load to the\n",
      ":func:`sklearn.datasets.fetch_20newsgroups` function::\n",
      "\n",
      "  >>> cats = ['alt.atheism', 'sci.space']\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
      "\n",
      "  >>> list(newsgroups_train.target_names)\n",
      "  ['alt.atheism', 'sci.space']\n",
      "  >>> newsgroups_train.filenames.shape\n",
      "  (1073,)\n",
      "  >>> newsgroups_train.target.shape\n",
      "  (1073,)\n",
      "  >>> newsgroups_train.target[:10]\n",
      "  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "\n",
      "Converting text to vectors\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "In order to feed predictive or clustering models with the text data,\n",
      "one first need to turn the text into vectors of numerical values suitable\n",
      "for statistical analysis. This can be achieved with the utilities of the\n",
      "``sklearn.feature_extraction.text`` as demonstrated in the following\n",
      "example that extract `TF-IDF`_ vectors of unigram tokens\n",
      "from a subset of 20news::\n",
      "\n",
      "  >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "  >>> categories = ['alt.atheism', 'talk.religion.misc',\n",
      "  ...               'comp.graphics', 'sci.space']\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "  ...                                       categories=categories)\n",
      "  >>> vectorizer = TfidfVectorizer()\n",
      "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "  >>> vectors.shape\n",
      "  (2034, 34118)\n",
      "\n",
      "The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\n",
      "components by sample in a more than 30000-dimensional space\n",
      "(less than .5% non-zero features)::\n",
      "\n",
      "  >>> vectors.nnz / float(vectors.shape[0])\n",
      "  159.01327...\n",
      "\n",
      ":func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which \n",
      "returns ready-to-use token counts features instead of file names.\n",
      "\n",
      ".. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/\n",
      ".. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf\n",
      "\n",
      "\n",
      "Filtering text for more realistic training\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "It is easy for a classifier to overfit on particular things that appear in the\n",
      "20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\n",
      "high F-scores, but their results would not generalize to other documents that\n",
      "aren't from this window of time.\n",
      "\n",
      "For example, let's look at the results of a multinomial Naive Bayes classifier,\n",
      "which is fast to train and achieves a decent F-score::\n",
      "\n",
      "  >>> from sklearn.naive_bayes import MultinomialNB\n",
      "  >>> from sklearn import metrics\n",
      "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "  ...                                      categories=categories)\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> clf = MultinomialNB(alpha=.01)\n",
      "  >>> clf.fit(vectors, newsgroups_train.target)\n",
      "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "  0.88213...\n",
      "\n",
      "(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\n",
      "the training and test data, instead of segmenting by time, and in that case\n",
      "multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\n",
      "yet of what's going on inside this classifier?)\n",
      "\n",
      "Let's take a look at what the most informative features are:\n",
      "\n",
      "  >>> import numpy as np\n",
      "  >>> def show_top10(classifier, vectorizer, categories):\n",
      "  ...     feature_names = np.asarray(vectorizer.get_feature_names())\n",
      "  ...     for i, category in enumerate(categories):\n",
      "  ...         top10 = np.argsort(classifier.coef_[i])[-10:]\n",
      "  ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
      "  ...\n",
      "  >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\n",
      "  alt.atheism: edu it and in you that is of to the\n",
      "  comp.graphics: edu in graphics it is for and of to the\n",
      "  sci.space: edu it that is in and space to of the\n",
      "  talk.religion.misc: not it you in is that and to of the\n",
      "\n",
      "\n",
      "You can now see many things that these features have overfit to:\n",
      "\n",
      "- Almost every group is distinguished by whether headers such as\n",
      "  ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\n",
      "- Another significant feature involves whether the sender is affiliated with\n",
      "  a university, as indicated either by their headers or their signature.\n",
      "- The word \"article\" is a significant feature, based on how often people quote\n",
      "  previous posts like this: \"In article [article ID], [name] <[e-mail address]>\n",
      "  wrote:\"\n",
      "- Other features match the names and e-mail addresses of particular people who\n",
      "  were posting at the time.\n",
      "\n",
      "With such an abundance of clues that distinguish newsgroups, the classifiers\n",
      "barely have to identify topics from text at all, and they all perform at the\n",
      "same high level.\n",
      "\n",
      "For this reason, the functions that load 20 Newsgroups data provide a\n",
      "parameter called **remove**, telling it what kinds of information to strip out\n",
      "of each file. **remove** should be a tuple containing any subset of\n",
      "``('headers', 'footers', 'quotes')``, telling it to remove headers, signature\n",
      "blocks, and quotation blocks respectively.\n",
      "\n",
      "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "  ...                                      remove=('headers', 'footers', 'quotes'),\n",
      "  ...                                      categories=categories)\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')\n",
      "  0.77310...\n",
      "\n",
      "This classifier lost over a lot of its F-score, just because we removed\n",
      "metadata that has little to do with topic classification.\n",
      "It loses even more if we also strip this metadata from the training data:\n",
      "\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "  ...                                       remove=('headers', 'footers', 'quotes'),\n",
      "  ...                                       categories=categories)\n",
      "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "  >>> clf = MultinomialNB(alpha=.01)\n",
      "  >>> clf.fit(vectors, newsgroups_train.target)\n",
      "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "  0.76995...\n",
      "\n",
      "Some other classifiers cope better with this harder version of the task. Try\n",
      "running :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py` with and without\n",
      "the ``--filter`` option to compare the results.\n",
      "\n",
      ".. topic:: Recommendation\n",
      "\n",
      "  When evaluating text classifiers on the 20 Newsgroups data, you\n",
      "  should strip newsgroup-related metadata. In scikit-learn, you can do this by\n",
      "  setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be\n",
      "  lower because it is more realistic.\n",
      "\n",
      ".. topic:: Examples\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(news.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T01:33:06.962727Z",
     "start_time": "2021-04-10T01:33:06.932162Z"
    }
   },
   "outputs": [],
   "source": [
    "# 训练集与测试集划分\n",
    "x_train, x_test, y_train, y_test = train_test_split(news.data, news.target, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T01:33:25.185000Z",
     "start_time": "2021-04-10T01:33:21.642942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ðòïäáöá', 'ñaustin', 'óáíïìåô', 'ýé']\n"
     ]
    }
   ],
   "source": [
    "# 文本特征提取\n",
    "tfidf = TfidfVectorizer()\n",
    "x_train = tfidf.fit_transform(x_train)\n",
    "\n",
    "print(tfidf.get_feature_names()[-5:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T01:33:44.499259Z",
     "start_time": "2021-04-10T01:33:43.315108Z"
    }
   },
   "outputs": [],
   "source": [
    "# 利用训练集数据计算测试集的tfidf值\n",
    "x_test = tfidf.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T01:34:48.496975Z",
     "start_time": "2021-04-10T01:34:48.304563Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 进行朴素贝叶斯算法的预测\n",
    "mlt = MultinomialNB(alpha=1.0)\n",
    "\n",
    "# 模型训练\n",
    "mlt.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T01:37:13.453568Z",
     "start_time": "2021-04-10T01:37:13.395726Z"
    }
   },
   "outputs": [],
   "source": [
    "# 预测\n",
    "y_pre = mlt.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T01:35:39.061368Z",
     "start_time": "2021-04-10T01:35:38.989446Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8376485568760611"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准确率\n",
    "mlt.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T01:38:45.648080Z",
     "start_time": "2021-04-10T01:38:45.595083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个类别的精确率和召回率：\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.85      0.71      0.77       191\n",
      "           comp.graphics       0.91      0.75      0.82       256\n",
      " comp.os.ms-windows.misc       0.81      0.90      0.85       234\n",
      "comp.sys.ibm.pc.hardware       0.74      0.81      0.78       246\n",
      "   comp.sys.mac.hardware       0.90      0.89      0.89       224\n",
      "          comp.windows.x       0.95      0.79      0.86       272\n",
      "            misc.forsale       0.93      0.69      0.79       254\n",
      "               rec.autos       0.91      0.90      0.91       249\n",
      "         rec.motorcycles       0.96      0.95      0.96       248\n",
      "      rec.sport.baseball       0.94      0.94      0.94       269\n",
      "        rec.sport.hockey       0.88      0.98      0.93       233\n",
      "               sci.crypt       0.75      0.98      0.85       247\n",
      "         sci.electronics       0.93      0.82      0.87       239\n",
      "                 sci.med       0.97      0.82      0.89       269\n",
      "               sci.space       0.86      0.95      0.91       246\n",
      "  soc.religion.christian       0.48      0.99      0.64       226\n",
      "      talk.politics.guns       0.73      0.97      0.83       220\n",
      "   talk.politics.mideast       0.92      0.96      0.94       220\n",
      "      talk.politics.misc       1.00      0.59      0.74       198\n",
      "      talk.religion.misc       0.95      0.11      0.19       171\n",
      "\n",
      "                accuracy                           0.84      4712\n",
      "               macro avg       0.87      0.83      0.82      4712\n",
      "            weighted avg       0.87      0.84      0.83      4712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"每个类别的精确率和召回率：\")\n",
    "print( classification_report(y_test, y_pre, target_names=news.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
